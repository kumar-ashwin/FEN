Making a new environment:
    add  super init
    self.state_variables = [] # List of state variable names (str). Will be used to get and set state
    self.fairness_vars 
    remove get_state and set_state
    add get_stateful_observation
    remove get_obs
    implement get_post_decision_state_agent
    remove get_post_decision_state
    implement compute_best_decision

For the job environment:
What works
1. Able to train with the agent constraint environment- you cannot move into a space that was occupied at t-1 (unless you stay)
    Params: --n_episode 1000
        --max_steps 100
        --learning_rate 0.0003
        Learns by about 200 episodes
        This does have a proximity reward, (-0.1*distance to job).

        // !Examine why the fairness fuction is not learning. Check the fairness reward, see if it works as expected.
        - Observation:
            !Simple state space seems to be pretty good. However, it misses the point about using more detailed fairness information
    // 1. The fairness rewards are tiny (~10^-17)
    2. Important note. The fairness reward was calculated assuming the agents immediately received the reward. 
        // !su_post was calculated in a hacky way for matthew. This might fail now!
    // 3. !TODO - The Agent's location when reset is badly done. There seems to be a bug, it is possible that 2 agents land in the same spot

 What does not work:
    Allowing all locations in the ILP with a resource constraint fails to learn to be fair.
    Also, there is basically no tradeoff for fairness, the default greedy strategy just lets agents switch in and out without cost

For matthew:
// 1. The validation utilities have this weird pattern: the last 4 agents consistently get higher utility??
    // Figure out why. There should be no reason for this to happen.
        // Well, that is because I made the environment to be that way. The last 4 agents are of a different type.
    2. higher lr seems beteter (0.0003)
    // 3. One way to make this faster, and the tradeoff more meaningful, is to reduce the number of steps per epoch.
    4. !In multi-agent, beta=0 learning is poor. Figure out why (this seems to be a general thing)
        - Option 1: If learning_beta=0, set learn_fairness to false. It is likely that the model still tries to update the fairness loss, as beta is used nowhere in the update
        - Option 2: If learning_beta=0, set the fairness reward to 0. This is likely the same as option 1, but it is a more direct approach: Suggested by copilot
        - Option 3: Change the update function to work based off the beta value. The fairness loss is scaled by beta?. This is likely the best approach, but it is also the most work. Will involve redoing all experiments.
    5. Generally, need to think about the fairness reward. Do we want the fair agent to learn from F(s) - (R_f + F(s')) or \beta*(F(s) - (R_f + F(s'))? Does it make a difference in the split agent? It should make a difference in the multi-head agent.
!Other thing to check: The num_samples is just 32 and num_min_samples is 1000/num_agents. Update freq is 100, so we are horribly underutilizing experiences

Plant domain:
    1. Fixed one issue with the reward not being properly transmitted (np array dtype issue)
    2. The model still does not learn anything
        Still does not learn anything if requirement is [0,1,0]. Should have been easier. 
        >0, but still very poor. Could have been ~40 in this case.
    3. Found issues with the post-decision state. It wasn't taking into account relative positions
        Still not learning.
    4. Removing 'loc' seems to make it worse.

For the paper:
1. The importance of past discounting and warm starts. Could use a section with some experiments about it.
2. A comparison of how good online SI is compared to the best learned approach. 
3. Value of using the exact breakdown by agent. 
    - This can be done for just one domain perhaps.
4. The job environment needs a shaping reward to help with learning.
5. A note on how this deals with adversarial reporting of utility:
    Can agents misreport their utility to get better allocations?
    - If agents lie about their utility, they might get their "preferred" allocation (which they lied about)
    - If agents try to over-report the benefit, they might be able to trick the system if they underreport their worst case
    - If agents can lie about their history, they might be able to get a better allocation
6. "Area under the pareto front" way to compare how well a fixed point trained model does at different beta values.
    Each beta will have one point representing the area under its pareto front.
7. What about a non-central approach? If each agent makes their own decision, does this training help?
8. Might need to run bootstraps to get average behavior at each beta.
!! None of the domains so far have a variable action space that prevents policy learning.

!? Why cant the networks predict all actions at once, and we superimpose them?

https://arxiv.org/pdf/2401.02552.pdf: Recent paper about long term fairness (2024)


Warm start env:
0. !Learning with 0 beta is not working
1. Changed the logging, to have "system_utility" instead of "utility". This could break logging and plotting
2. Have a generalized template for environments. Need to split system and agent utility, to allow for fairness to be less strongly coupled
3. A TODO: Convert all domains to the same general format:
    - !Post decision states calculated by taking a step and then resetting the environment
        - NO!. THis is wrong. PD state should not include the reward from the next state, because of the learning scheme.
        - Learning: loss = Q(s^) - (R(s,a) + Q(s'^))
            s^ = s + a, s'^ = s^ + a*
            If s^ includes R(s,a), then this equation only works if we use R(s',a*))
        - The PD state should definitely include rewards, but it should not include the reward from the current action.
    - Separate utility and fairness rewards. 
    - rename discounted_su and su. Bad naming convention so far.
    - 
4. An issue with the normalized discounted_su is that the magnitude is too small

Come up with a way to scale the f_reward automatically to be in the correct range?


QOL improvements
Sync this to a gym env setting
! Look into imitation learning
// ! TODO: For final validation, load best model. Will make life easier
// Add a loader for environment, and make environment arguments part of argument parser
    // Will allow a single file to be used for all experiments
// Create an evaluation file. Load best models and evalaute them


Environment updates:
// 1. create a function to get the fairness rewads distribution
// 2. Create a function to get the features for each agent. This will be different for each environment
// 3. Do I need set_state in agent update?? Could just save new_obs in experience?
//     Yes, because the trasnsition needs to be computed in the agent update. Transition is not computed using Observation

!! Important fix: All agents need to update from combined experiences not just one at a time

For agents:
1. !! There seems to be no reason to use Huber loss, so changing it to MSE.
2. !! TODO: Update the classes to tf2
3. Updated all agent classes to only back propagate once with all experiences
