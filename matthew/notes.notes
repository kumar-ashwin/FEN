For the job environment:
What works
1. Able to train with the agent constraint environment- you cannot move into a space that was occupied at t-1 (unless you stay)
    Params: --n_episode 1000
        --max_steps 100
        --learning_rate 0.0003
        Learns by about 200 episodes
        This does have a proximity reward, (-0.1*distance to job).

        // !Examine why the fairness fuction is not learning. Check the fairness reward, see if it works as expected.
        - Observation:
            !Simple state space seems to be pretty good. However, it misses the point about using more detailed fairness information
    // 1. The fairness rewards are tiny (~10^-17)
    2. Important note. The fairness reward was calculated assuming the agents immediately received the reward. 
        // !su_post was calculated in a hacky way for matthew. This might fail now!
    3. !TODO - The Agent's location when reset is badly done. There seems to be a bug, it is possible that 2 agents land in the same spot

 What does not work:
    Allowing all locations in the ILP with a resource constraint fails to learn to be fair.
    Also, there is basically no tradeoff for fairness, the default greedy strategy just lets agents switch in and out without cost

For matthew:
1. The validation utilities have this weird pattern: the last 4 agents consistently get higher utility??
    Figure out why. There should be no reason for this to happen.
        Well, that is because I made the environment to be that way. The last 4 agents are of a different type.
    2. higher lr seems beteter (0.0003)
    3. One way to make this faster, and the tradeoff more meaningful, is to reduce the number of steps per epoch.
    4. !In multi-agent, beta=0 learning is poor. Figure out why (this seems to be a general thing)
        - Option 1: If learning_beta=0, set learn_fairness to false. It is likely that the model still tries to update the fairness loss, as beta is used nowhere in the update
        - Option 2: If learning_beta=0, set the fairness reward to 0. This is likely the same as option 1, but it is a more direct approach: Suggested by copilot
        - Option 3: Change the update function to work based off the beta value. The fairness loss is scaled by beta?. This is likely the best approach, but it is also the most work. Will involve redoing all experiments.
    5. Generally, need to think about the fairness reward. Do we want the fair agent to learn from F(s) - (R_f + F(s')) or \beta*(F(s) - (R_f + F(s'))? Does it make a difference in the split agent? It should make a difference in the multi-head agent.
!Other thing to check: The num_samples is just 32 and num_min_samples is 1000/num_agents. Update freq is 100, so we are horribly underutilizing experiences


For the paper:
1. The importance of past discounting and warm starts. Could use a section with some experiments about it.
2. A comparison of how good online SI is compared to the best learned approach. 
3. Value of using the exact breakdown by agent. 
    - This can be done for just one domain perhaps.
4. The job environment needs a shaping reward to help with learning.
5. A note on how this deals with adversarial reporting of utility:
    Can agents misreport their utility to get better allocations?
    - If agents lie about their utility, they might get their "preferred" allocation (which they lied about)
    - If agents try to over-report the benefit, they might be able to trick the system if they underreport their worst case
    - If agents can lie about their history, they might be able to get a better allocation
6. "Area under the pareto front" way to compare how well a fixed point trained model does at different beta values.
    Each beta will have one point representing the area under its pareto front.
7. What about a non-central approach? If each agent makes their own decision, does this training help?
8. Might need to run bootstraps to get average behavior at each beta.

!? Why cant the networks predict all actions at once, and we superimpose them?

https://arxiv.org/pdf/2401.02552.pdf: Recent paper about long term fairness (2024)


Warm start env:
0. !Learning with 0 beta is not working
1. Changed the logging, to have "system_utility" instead of "utility". This could break logging and plotting
2. Have a generalized template for environments. Need to split system and agent utility, to allow for fairness to be less strongly coupled
3. A TODO: Convert all domains to the same general format:
    - !Post decision states calculated by taking a step and then resetting the environment
        - NO!. THis is wrong. PD state should not include the reward from the next state, because of the learning scheme.
        - Learning: loss = Q(s^) - (R(s,a) + Q(s'^))
            s^ = s + a, s'^ = s^ + a*
            If s^ includes R(s,a), then this equation only works if we use R(s',a*))
        - The PD state should definitely include rewards, but it should not include the reward from the current action.
    - Separate utility and fairness rewards. 
    - rename discounted_su and su. Bad naming convention so far.
